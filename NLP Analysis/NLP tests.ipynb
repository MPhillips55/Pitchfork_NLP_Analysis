{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate spacy parser\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pull out sample text from review database\n",
    "df = pd.read_csv('content.csv')\n",
    "sample = df.content\n",
    "sample = sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse sample text with spacy object\n",
    "parsed_review = nlp(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "“Trip-hop” eventually became a ’90s punchline, a music-press shorthand for “overhyped hotel lounge music.\n",
      "\n",
      "Sentence 2:\n",
      "” But today, the much-maligned subgenre almost feels like a secret precedent.\n",
      "\n",
      "Sentence 3:\n",
      "Listen to any of the canonical Bristol-scene albums of the mid-late ’90s, when the genre was starting to chafe against its boundaries, and you’d think the claustrophobic, anxious 21st century started a few years ahead of schedule.\n",
      "\n",
      "Sentence 4:\n",
      "Looked at from the right angle, trip-hop is part of an unbroken chain that runs from the abrasion of ’80s post-punk to the ruminative pop-R&B-dance fusion of the moment. \n",
      "\n",
      "Sentence 5:\n",
      "The best of it has aged far more gracefully (and forcefully) than anything recorded in the waning days of the record industry’s pre-filesharing monomania has any right to.\n",
      "\n",
      "Sentence 6:\n",
      "Tricky rebelled against being attached at the hip to a scene he was already looking to shed and decamped for Jamaica to record a more aggressive, bristling-energy mutation of his style in ’96; the name Pre-Millennium Tension is the only obvious thing that tells you it’s two decades old rather than two weeks.\n",
      "\n",
      "Sentence 7:\n",
      "And Portishead’s ’97 self-titled saw the stress-fractured voice of Beth Gibbons envisioning romance as codependent, mutually assured destruction while Geoff Barrow sunk into his RZA-noir beats like The Conversation’s Gene Hackman ruminating over his surveillance tapes.\n",
      "\n",
      "Sentence 8:\n",
      "This was raw-nerved music, too single-minded and intense to carry an obvious timestamp. \n",
      "\n",
      "Sentence 9:\n",
      "But Massive Attack were the origin point of the trip-hop movement they and their peers were striving to escape the orbit of, and they nearly tore themselves to shreds in the process. Instead— or maybe as a result—they laid down their going-nova genre's definitive paranoia statement with Mezzanine.\n",
      "\n",
      "Sentence 10:\n",
      "The band's third album (not counting the Mad Professor-remixed No Protection) completes the last in a sort of de facto Bristol trilogy, where Tricky’s youthful iconoclasm and Portishead’s deep-focus emotional intensity set the scene for Massive Attack’s sense of near-suffocating dread.\n",
      "\n",
      "Sentence 11:\n",
      "The album corroded their tendencies to make big-wheel hymnals of interconnected lives where hope and despair trade precedent—on Mezzanine, it’s alienation all the way down.\n",
      "\n",
      "Sentence 12:\n",
      "There’s no safety from harm here, nothing you’ve got to be thankful for, nobody to take the force of the blow: what Mezzanine provides instead is a succession of parties and relationships and panopticons where the walls won’t stop closing in.\n",
      "\n",
      "Sentence 13:\n",
      "The lyrics establish this atmosphere all on their own.\n",
      "\n",
      "Sentence 14:\n",
      "Sex, in “Inertia Creeps,” is reduced to a meeting of “two undernourished egos, four rotating hips,” the focus of a failing relationship that's left its participants too numbed with their own routine dishonesty to break it off.\n",
      "\n",
      "Sentence 15:\n",
      "The voice singing it—Massive Attack's cornerstone co-writer/producer Robert “3D” Del Naja—is raspy from exhaustion.\n",
      "\n",
      "Sentence 16:\n",
      "“\n",
      "\n",
      "Sentence 17:\n",
      "Dissolved Girl” reiterates this theme from the perspective of guest vocalist Sarah Jay Hawley (“Passion’s overrated anyway”).\n",
      "\n",
      "Sentence 18:\n",
      "On “\n",
      "\n",
      "Sentence 19:\n",
      "Risingson,” Grant “Daddy G”\n",
      "\n",
      "Sentence 20:\n",
      "Marshall nails the boredom and anxiety of being stuck somewhere you can’t stand with someone you’re starting to feel the same way about (“Why you want to take me to this party and breathe/I’m dying to leave/Every time we grind you know we severed lines”).\n",
      "\n",
      "Sentence 21:\n",
      "But Mezzanine’s defining moments come from guest vocalists who were famous long before Massive Attack even released their first album.\n",
      "\n",
      "Sentence 22:\n",
      "Horace Andy was already a legend in reggae circles, but his collaborations with Massive Attack gave him a wider crossover exposure, and all three of his appearances on Mezzanine are homages or nods to songs he'd charted with in his early-’70s come-up.\n",
      "\n",
      "Sentence 23:\n",
      "“\n",
      "\n",
      "Sentence 24:\n",
      "Angel” is a loose rewrite of his 1973 single “\n",
      "\n",
      "Sentence 25:\n",
      "You Are My Angel,” but it’s a fakeout after the first verse—originally a vision of beauty (“Come from way above/To bring me love”), transformed into an Old Testament avenger: “\n",
      "\n",
      "Sentence 26:\n",
      "On the dark side/Neutralize every man in sight.\n",
      "\n",
      "Sentence 27:\n",
      "”\n",
      "\n",
      "Sentence 28:\n",
      "The parenthetically titled, album-closing reprise of “(Exchange)” is a ghostly invocation of Andy’s “See\n",
      "\n",
      "Sentence 29:\n",
      "A Man's Face” cleverly disguised as a comedown track.\n",
      "\n",
      "Sentence 30:\n",
      "And then there’s “\n",
      "\n",
      "Sentence 31:\n",
      "Man Next Door,” the John Holt standard that Andy had previously recorded as “\n",
      "\n",
      "Sentence 32:\n",
      "Quiet Place”—on Mezzanine, it sounds less like an overheard argument from the next apartment over and more like a close-quarters reckoning with violence heard through thin walls ready to break.\n",
      "\n",
      "Sentence 33:\n",
      "It’s Andy at his emotionally nuanced and evocative best.\n",
      "\n",
      "Sentence 34:\n",
      "The other outside vocalist was even more of a coup: Liz Fraser, the singer and songwriter of Cocteau Twins, lends her virtuoso soprano to three songs that feel like exorcisms of the personal strife accompanying her band\n",
      "\n",
      "Sentence 35:\n",
      "’s breakup.\n",
      "\n",
      "Sentence 36:\n",
      "Her voice serves as an ethereal counterpoint to speaker-rattling production around it.\n",
      "\n",
      "Sentence 37:\n",
      "“\n",
      "\n",
      "Sentence 38:\n",
      "Black Milk” contains the album’s most spiritually unnerving words (“Eat me/In the space/Within my heart/Love you for God/Love you for the Mother”), even as her lead and the elegiac beat make for some of its most beautiful sounds.\n",
      "\n",
      "Sentence 39:\n",
      "She provides the wistful counterpoint to the night-shift alienation of “Group Four.”\n",
      "\n",
      "Sentence 40:\n",
      "And then there's “Teardrop,”\n",
      "\n",
      "Sentence 41:\n",
      "her finest moment on the album.\n",
      "\n",
      "Sentence 42:\n",
      "Legend has it the song was briefly considered for Madonna; Andrew “Mushroom” Vowles sent the demo to her, but was overruled by Daddy G and 3D, who both wanted Fraser.\n",
      "\n",
      "Sentence 43:\n",
      "Democracy thankfully worked this time around, as Fraser’s performance—recorded in part on the day she discovered that Jeff Buckley, who she’d had an estranged working relationship and friendship with, had drowned in Memphis’\n",
      "\n",
      "Sentence 44:\n",
      "Wolf River—was a heart-rending performance that gave Massive Attack their first (and so far only)\n",
      "\n",
      "Sentence 45:\n",
      "UK Top 10 hit.\n",
      "\n",
      "Sentence 46:\n",
      "Originally set for a late ’97 release, Mezzanine got pushed back four months because Del Naja refused to stop reworking the tracks, tearing them apart and rebuilding them until they’re so polished they gleam.\n",
      "\n",
      "Sentence 47:\n",
      "It sure sounds like the product of bloody-knuckled labor, all that empty-space reverb and melted-together multitrack vocals and oppressive low-end.\n",
      "\n",
      "Sentence 48:\n",
      "(The first sound you hear on the album, that lead-jointed bassline on “\n",
      "\n",
      "Sentence 49:\n",
      "Angel,” is to subwoofers what “\n",
      "\n",
      "Sentence 50:\n",
      "Planet Earth” \n",
      "\n",
      "Sentence 51:\n",
      "is to high-def television.)\n",
      "\n",
      "Sentence 52:\n",
      "But it also groans with the burden of creative conflict, a working process that created rifts between Del Naja and Vowles, who left shortly after Mezzanine dropped following nearly 15 years of collaboration.\n",
      "\n",
      "Sentence 53:\n",
      "Mezzanine began the band’s relationship with producer Neil Davidge, who’d known Vowles dating back to the early ’90s and met the rest of the band after the completion of Protection.\n",
      "\n",
      "Sentence 54:\n",
      "He picked a chaotic time to jump in, but Davidge and 3D forged a creative bond working through that pressure. \n",
      "\n",
      "Sentence 55:\n",
      "Mezzanine was a document of unity, not fragmentation.\n",
      "\n",
      "Sentence 56:\n",
      "Despite their rifts, they were a post-genre outfit, one that couldn’t separate dub from punk from hip-hop from R&B because the basslines all worked together and because classifications are for toe tags.\n",
      "\n",
      "Sentence 57:\n",
      "All their acknowledged samples—including the joy-buzzer synths from Ultravox’s “Rockwrok” (“Inertia Creeps”), the opulent ache of Isaac Hayes’ celestial-soul take on “\n",
      "\n",
      "Sentence 58:\n",
      "Our Day Will Come” (“Exchange”), Robert Smith’s nervous “tick tick tick” from the Cure’s “10:15 Saturday Night,” and the most concrete-crumbling throwdown of the Led Zep “Levee” break ever deployed (the latter two on “\n",
      "\n",
      "Sentence 59:\n",
      "Man\n",
      "\n",
      "Sentence 60:\n",
      "Next Door”)—were sourced from  1968 and 1978, well-traveled crate-digging territory.\n",
      "\n",
      "Sentence 61:\n",
      "But what they build from that is its own beast.  \n",
      "\n",
      "Sentence 62:\n",
      "Their working method never got any faster.\n",
      "\n",
      "Sentence 63:\n",
      "The four-year gap between Protection and Mezzanine became a five-year gap until 2003’s 100th Window, then another seven years between that record and 2010’s Heligoland, plus another seven years and counting with no full-lengths to show for it.\n",
      "\n",
      "Sentence 64:\n",
      "Not that they've been slacking: we've gotten a multimedia film/music collaboration with Adam Curtis, the respectable but underrated Ritual Spirit EP, and Del Naja’s notoriously rumored side gig as Banksy.\n",
      "\n",
      "Sentence 65:\n",
      "(Hey, 3D does have a background in graffiti art.)\n",
      "\n",
      "Sentence 66:\n",
      "But the ordeal of both recording and touring Mezzanine took its own toll.\n",
      "\n",
      "Sentence 67:\n",
      "A late ’98 interview with Del Naja saw him optimistic about its reputation-shedding style: “\n",
      "\n",
      "Sentence 68:\n",
      "I always said it was for the greater good of the fucking project because if this album was a bit different from the last two, the next one would be even freer to be whatever it wants to be.”\n",
      "\n",
      "Sentence 69:\n",
      "But fatigue and restlessness rarely make for a productive mixture, and that same spark of tension which carried Mezzanine over the threshold proved unsustainable, not just for Massive Attack’s creativity but their continued existence.\n",
      "\n",
      "Sentence 70:\n",
      "Still, it’s hard not to feel the album’s legacy resonating elsewhere—and not just in “Teardrop” becoming the cue for millions of TV viewers to brace themselves for Hugh Laurie’s cranky-genius-doctor schtick. \n",
      "\n",
      "Sentence 71:\n",
      "Graft its tense feelings of nervy isolation and late-night melancholy onto two-step, and you’re partway to the blueprint for Plastician and Burial.\n",
      "\n",
      "Sentence 72:\n",
      "You can hear flashes of that mournful romantic alienation in James Blake, the graceful, bass-riddled emotional abrasion in FKA twigs, the all-absorbing post-genre rock/soul ambitions in Young Fathers or Algiers.\n",
      "\n",
      "Sentence 73:\n",
      "Mezzanine stands as an album built around echoes of the ’70s, wrestled through the immediacy of its creators' tumultuous late ’90s, and fearless enough that it still sounds like it belongs in whatever timeframe you're playing it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spacy can separate out sentences as below\n",
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print('Sentence {}:'.format(num+1))\n",
    "    print(sentence)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: today - DATE\n",
      "\n",
      "Entity 2: Bristol - GPE\n",
      "\n",
      "Entity 3: 21st century - DATE\n",
      "\n",
      "Entity 4: Jamaica - GPE\n",
      "\n",
      "Entity 5: ’96 - GPE\n",
      "\n",
      "Entity 6: two decades - DATE\n",
      "\n",
      "Entity 7: two weeks - DATE\n",
      "\n",
      "Entity 8: Portishead’s - ORG\n",
      "\n",
      "Entity 9: Beth Gibbons - PERSON\n",
      "\n",
      "Entity 10: Geoff Barrow - PERSON\n",
      "\n",
      "Entity 11: The Conversation’s Gene Hackman - WORK_OF_ART\n",
      "\n",
      "Entity 12: Mezzanine - PERSON\n",
      "\n",
      "Entity 13: third - ORDINAL\n",
      "\n",
      "Entity 14: Bristol - GPE\n",
      "\n",
      "Entity 15: Tricky’s - PERSON\n",
      "\n",
      "Entity 16: Portishead’s - PERSON\n",
      "\n",
      "Entity 17: Mezzanine - PERSON\n",
      "\n",
      "Entity 18: “Inertia Creeps - ORG\n",
      "\n",
      "Entity 19: two - CARDINAL\n",
      "\n",
      "Entity 20: four - CARDINAL\n",
      "\n",
      "Entity 21: Robert “3D - PERSON\n",
      "\n",
      "Entity 22: Del Naja—is - PERSON\n",
      "\n",
      "Entity 23: Sarah Jay - PERSON\n",
      "\n",
      "Entity 24: Risingson - PERSON\n",
      "\n",
      "Entity 25: Grant “ - PERSON\n",
      "\n",
      "Entity 26: Marshall - PERSON\n",
      "\n",
      "Entity 27: Mezzanine - NORP\n",
      "\n",
      "Entity 28: first - ORDINAL\n",
      "\n",
      "Entity 29: Horace Andy  - PERSON\n",
      "\n",
      "Entity 30: three - CARDINAL\n",
      "\n",
      "Entity 31: early-’70s - CARDINAL\n",
      "\n",
      "Entity 32: Angel” - PERSON\n",
      "\n",
      "Entity 33: 1973 - DATE\n",
      "\n",
      "Entity 34: Angel - PERSON\n",
      "\n",
      "Entity 35: first - ORDINAL\n",
      "\n",
      "Entity 36: “(Exchange - ORG\n",
      "\n",
      "Entity 37: Andy’s - PERSON\n",
      "\n",
      "Entity 38: ’s “ - ORG\n",
      "\n",
      "Entity 39: John Holt - PERSON\n",
      "\n",
      "Entity 40: Andy - PERSON\n",
      "\n",
      "Entity 41: Andy - PERSON\n",
      "\n",
      "Entity 42: Liz Fraser - PERSON\n",
      "\n",
      "Entity 43: Cocteau Twins - ORG\n",
      "\n",
      "Entity 44: three - CARDINAL\n",
      "\n",
      "Entity 45: Teardrop - ORG\n",
      "\n",
      "Entity 46: Madonna - PERSON\n",
      "\n",
      "Entity 47: Andrew “Mushroom - PERSON\n",
      "\n",
      "Entity 48: Daddy G - ORG\n",
      "\n",
      "Entity 49: Fraser - PERSON\n",
      "\n",
      "Entity 50: Fraser’s - PERSON\n",
      "\n",
      "Entity 51: Jeff Buckley - PERSON\n",
      "\n",
      "Entity 52: Memphis - GPE\n",
      "\n",
      "Entity 53: Wolf River—was - PERSON\n",
      "\n",
      "Entity 54: first - ORDINAL\n",
      "\n",
      "Entity 55: Mezzanine - ORG\n",
      "\n",
      "Entity 56: four months - DATE\n",
      "\n",
      "Entity 57: Del Naja - PERSON\n",
      "\n",
      "Entity 58: first - ORDINAL\n",
      "\n",
      "Entity 59: Angel - PERSON\n",
      "\n",
      "Entity 60: Del Naja - PERSON\n",
      "\n",
      "Entity 61: Vowles - PERSON\n",
      "\n",
      "Entity 62: Mezzanine - PERSON\n",
      "\n",
      "Entity 63: nearly 15 years - DATE\n",
      "\n",
      "Entity 64: Neil Davidge - PERSON\n",
      "\n",
      "Entity 65: Vowles - PERSON\n",
      "\n",
      "Entity 66: Davidge - PERSON\n",
      "\n",
      "Entity 67: Ultravox - GPE\n",
      "\n",
      "Entity 68: Isaac Hayes - PERSON\n",
      "\n",
      "Entity 69: Robert Smith - PERSON\n",
      "\n",
      "Entity 70: Saturday - DATE\n",
      "\n",
      "Entity 71: the latter - DATE\n",
      "\n",
      "Entity 72: 1968 - DATE\n",
      "\n",
      "Entity 73: 1978 - DATE\n",
      "\n",
      "Entity 74: four-year - DATE\n",
      "\n",
      "Entity 75: five-year - DATE\n",
      "\n",
      "Entity 76: 2003 - DATE\n",
      "\n",
      "Entity 77: 2010 - DATE\n",
      "\n",
      "Entity 78: Adam Curtis - PERSON\n",
      "\n",
      "Entity 79: Del Naja - PERSON\n",
      "\n",
      "Entity 80: Banksy - PERSON\n",
      "\n",
      "Entity 81: Mezzanine - ORG\n",
      "\n",
      "Entity 82: Del Naja - PERSON\n",
      "\n",
      "Entity 83: the last two - DATE\n",
      "\n",
      "Entity 84: Mezzanine - PERSON\n",
      "\n",
      "Entity 85: millions - CARDINAL\n",
      "\n",
      "Entity 86: Hugh Laurie’s - PERSON\n",
      "\n",
      "Entity 87: late-night - TIME\n",
      "\n",
      "Entity 88: two - CARDINAL\n",
      "\n",
      "Entity 89: Plastician - NORP\n",
      "\n",
      "Entity 90: Burial - GPE\n",
      "\n",
      "Entity 91: James Blake - PERSON\n",
      "\n",
      "Entity 92: Algiers - GPE\n",
      "\n",
      "Entity 93: late ’90s - TIME\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# named entity detection\n",
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print('Entity {}:'.format(num+1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trip</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hop</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token_text part_of_speech\n",
       "0          “          PUNCT\n",
       "1       Trip           NOUN\n",
       "2          -          PUNCT\n",
       "3        hop           NOUN\n",
       "4          ”          PUNCT"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token identification\n",
    "\n",
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "token_df = pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "            columns=['token_text', 'part_of_speech'])\n",
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“</td>\n",
       "      <td>\"</td>\n",
       "      <td>“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trip</td>\n",
       "      <td>trip</td>\n",
       "      <td>Xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hop</td>\n",
       "      <td>hop</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "      <td>\"</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eventually</td>\n",
       "      <td>eventually</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>became</td>\n",
       "      <td>become</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>’90s</td>\n",
       "      <td>’90s</td>\n",
       "      <td>’ddx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>punchline</td>\n",
       "      <td>punchline</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>press</td>\n",
       "      <td>press</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shorthand</td>\n",
       "      <td>shorthand</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>“</td>\n",
       "      <td>\"</td>\n",
       "      <td>“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>overhyped</td>\n",
       "      <td>overhyped</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hotel</td>\n",
       "      <td>hotel</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text token_lemma token_shape\n",
       "0            “           \"           “\n",
       "1         Trip        trip        Xxxx\n",
       "2            -           -           -\n",
       "3          hop         hop         xxx\n",
       "4            ”           \"           ”\n",
       "5   eventually  eventually        xxxx\n",
       "6       became      become        xxxx\n",
       "7            a           a           x\n",
       "8         ’90s        ’90s        ’ddx\n",
       "9    punchline   punchline        xxxx\n",
       "10           ,           ,           ,\n",
       "11           a           a           x\n",
       "12       music       music        xxxx\n",
       "13           -           -           -\n",
       "14       press       press        xxxx\n",
       "15   shorthand   shorthand        xxxx\n",
       "16         for         for         xxx\n",
       "17           “           \"           “\n",
       "18   overhyped   overhyped        xxxx\n",
       "19       hotel       hotel        xxxx"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text normalization, stemming/lemmatization\n",
    "# shape analysis\n",
    "\n",
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "nor_df = pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "                     columns=['token_text', 'token_lemma', 'token_shape'])\n",
    "nor_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trip</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hop</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eventually</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>became</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>’90s</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>punchline</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>music</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>press</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shorthand</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>“</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>overhyped</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hotel</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text entity_type inside_outside_begin\n",
       "0            “                                O\n",
       "1         Trip                                O\n",
       "2            -                                O\n",
       "3          hop                                O\n",
       "4            ”                                O\n",
       "5   eventually                                O\n",
       "6       became                                O\n",
       "7            a                                O\n",
       "8         ’90s                                O\n",
       "9    punchline                                O\n",
       "10           ,                                O\n",
       "11           a                                O\n",
       "12       music                                O\n",
       "13           -                                O\n",
       "14       press                                O\n",
       "15   shorthand                                O\n",
       "16         for                                O\n",
       "17           “                                O\n",
       "18   overhyped                                O\n",
       "19       hotel                                O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token-level entity analysis\n",
    "\n",
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "ent_df = pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "                     columns=['token_text', 'entity_type', 'inside_outside_begin'])\n",
    "ent_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_proba</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“</td>\n",
       "      <td>-9.795314</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trip</td>\n",
       "      <td>-13.672223</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.468655</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hop</td>\n",
       "      <td>-10.939725</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "      <td>-9.812149</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eventually</td>\n",
       "      <td>-9.494384</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>became</td>\n",
       "      <td>-9.810510</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.929788</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>’90s</td>\n",
       "      <td>-18.391684</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>punchline</td>\n",
       "      <td>-12.871519</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.454960</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.929788</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>music</td>\n",
       "      <td>-8.735067</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.468655</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>press</td>\n",
       "      <td>-10.130120</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shorthand</td>\n",
       "      <td>-13.740866</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td>-4.880109</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>“</td>\n",
       "      <td>-9.795314</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>overhyped</td>\n",
       "      <td>-13.863889</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hotel</td>\n",
       "      <td>-11.047312</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lounge</td>\n",
       "      <td>-12.681898</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>music</td>\n",
       "      <td>-8.735067</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.067898</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>”</td>\n",
       "      <td>-9.812149</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>But</td>\n",
       "      <td>-7.014297</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>today</td>\n",
       "      <td>-8.664745</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.454960</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.528767</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>much</td>\n",
       "      <td>-6.584302</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.468655</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  log_proba stop? punctuation? whitespace? number?  \\\n",
       "0            “  -9.795314                Yes                       \n",
       "1         Trip -13.672223                                          \n",
       "2            -  -5.468655                Yes                       \n",
       "3          hop -10.939725                                          \n",
       "4            ”  -9.812149                Yes                       \n",
       "5   eventually  -9.494384                                          \n",
       "6       became  -9.810510   Yes                                    \n",
       "7            a  -3.929788   Yes                                    \n",
       "8         ’90s -18.391684                                          \n",
       "9    punchline -12.871519                                          \n",
       "10           ,  -3.454960                Yes                       \n",
       "11           a  -3.929788   Yes                                    \n",
       "12       music  -8.735067                                          \n",
       "13           -  -5.468655                Yes                       \n",
       "14       press -10.130120                                          \n",
       "15   shorthand -13.740866                                          \n",
       "16         for  -4.880109   Yes                                    \n",
       "17           “  -9.795314                Yes                       \n",
       "18   overhyped -13.863889                                          \n",
       "19       hotel -11.047312                                          \n",
       "20      lounge -12.681898                                          \n",
       "21       music  -8.735067                                          \n",
       "22           .  -3.067898                Yes                       \n",
       "23           ”  -9.812149                Yes                       \n",
       "24         But  -7.014297   Yes                                    \n",
       "25       today  -8.664745                                          \n",
       "26           ,  -3.454960                Yes                       \n",
       "27         the  -3.528767   Yes                                    \n",
       "28        much  -6.584302   Yes                                    \n",
       "29           -  -5.468655                Yes                       \n",
       "\n",
       "   out of vocab.?  \n",
       "0                  \n",
       "1                  \n",
       "2                  \n",
       "3                  \n",
       "4                  \n",
       "5                  \n",
       "6                  \n",
       "7                  \n",
       "8                  \n",
       "9                  \n",
       "10                 \n",
       "11                 \n",
       "12                 \n",
       "13                 \n",
       "14                 \n",
       "15                 \n",
       "16                 \n",
       "17                 \n",
       "18                 \n",
       "19                 \n",
       "20                 \n",
       "21                 \n",
       "22                 \n",
       "23                 \n",
       "24                 \n",
       "25                 \n",
       "26                 \n",
       "27                 \n",
       "28                 \n",
       "29                 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                    token.prob,\n",
    "                    token.is_stop,\n",
    "                    token.is_punct,\n",
    "                    token.is_space,\n",
    "                    token.like_num,\n",
    "                    token.is_oov)\n",
    "                   for token in parsed_review]\n",
    "\n",
    "df1 = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                          'log_proba',\n",
    "                          'stop?',\n",
    "                          'punctuation?',\n",
    "                          'whitespace?',\n",
    "                          'number?',\n",
    "                          'out of vocab.?'])\n",
    "\n",
    "def fill_in(x):\n",
    "    if x:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df1.loc[:, 'stop?':'out of vocab.?'] = (df1.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "df1.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda2\\envs\\py3DataScience\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "review_text_path = os.path.join('review_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = os.path.join('unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "if 0 == 1:\n",
    "    with codecs.open(unigram_sentences, 'w', encoding='utf-8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_text_path):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_parser = LineSentence(unigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip hop eventually become a ’90s punchline a music press shorthand for overhyped hotel lounge music\n",
      "\n",
      "but today the much malign subgenre almost feel like a secret precedent\n",
      "\n",
      "listen to any of the canonical bristol scene album of the mid late ’90s when the genre be start to chafe against its boundary and you’d think the claustrophobic anxious 21st century start a few year ahead of schedule\n",
      "\n",
      "look at from the right angle trip hop be part of an unbroken chain that run from the abrasion of ’80s post punk to the ruminative pop r&b dance fusion of the moment\n",
      "\n",
      "the best of it have age far more gracefully and forcefully than anything record in the waning day of the record industry ’s pre filesharing monomania have any right to\n",
      "\n",
      "tricky rebel against be attach at the hip to a scene he be already look to shed and decamp for jamaica to record a more aggressive bristle energy mutation of his style in ’96 the name pre millennium tension be the only obvious thing that tell you it ’s two decade old rather than two week\n",
      "\n",
      "and portishead ’s ’97 self title saw the stress fractured voice of beth gibbons envision romance as codependent mutually assured destruction while geoff barrow sink into his rza noir beat like the conversation ’s gene hackman ruminate over his surveillance tape\n",
      "\n",
      "this be raw nerved music too single minded and intense to carry an obvious timestamp\n",
      "\n",
      "but massive attack be the origin point of the trip hop movement they and their peer be strive to escape the orbit of and they nearly tear themselves to shred in the process instead— or maybe as a result—they lay down their go nova genre 's definitive paranoia statement with mezzanine\n",
      "\n",
      "the band 's third album not count the mad professor remixed no protection complete the last in a sort of de facto bristol trilogy where tricky ’s youthful iconoclasm and portishead ’s deep focus emotional intensity set the scene for massive attack ’s sense of near suffocate dread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "for unigram_sentence in it.islice(unigram_sentences_parser, 0, 10):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join('bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases(unigram_sentences_parser)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join('bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda2\\envs\\py3DataScience\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "if 1 == 1:\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf-8') as f:\n",
    "        for unigram_sentence in unigram_sentences_parser:\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_hop eventually_become a ’90s punchline a music press shorthand_for overhyped hotel_lounge music\n",
      "\n",
      "but today the much_malign subgenre almost feel_like a secret precedent\n",
      "\n",
      "listen to any of the canonical bristol scene album of the mid late_’90s when the genre be start to chafe_against its boundary and you’d_think the claustrophobic anxious 21st_century start a few_year ahead of schedule\n",
      "\n",
      "look_at from the right_angle trip_hop be part of an unbroken chain that run from the abrasion of ’80s post_punk to the ruminative pop r&b dance fusion of the moment\n",
      "\n",
      "the best of it have age far more gracefully and forcefully than_anything record in the waning_day of the record industry ’s pre filesharing monomania have any right to\n",
      "\n",
      "tricky rebel_against be attach at the hip to a scene he be already look to shed and decamp for jamaica to record a more aggressive bristle energy mutation of his style in ’96 the name pre_millennium tension be the only obvious thing that tell you it ’s two_decade old rather_than two week\n",
      "\n",
      "and portishead ’s ’97 self_title saw the stress fractured voice of beth_gibbons envision romance as codependent mutually_assured destruction while geoff_barrow sink_into his rza noir beat like the conversation ’s gene hackman ruminate over his surveillance tape\n",
      "\n",
      "this be raw_nerved music too single_minded and intense to carry an obvious timestamp\n",
      "\n",
      "but massive_attack be the origin point of the trip_hop movement they and their peer be strive to escape the orbit of and they nearly tear themselves to shred in the process instead— or maybe as a result—they lay_down their go nova genre 's definitive paranoia statement with mezzanine\n",
      "\n",
      "the band 's third_album not count the mad_professor remixed no protection complete the last in a sort of de_facto bristol trilogy where tricky ’s youthful iconoclasm and portishead ’s deep focus emotional_intensity set the scene for massive_attack ’s sense of near suffocate dread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 0, 10):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join('trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "trigram_model.save(trigram_model_filepath)\n",
    "\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join('trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_hop eventually_become a ’90s punchline a music press shorthand_for overhyped hotel_lounge music\n",
      "\n",
      "but today the much_malign subgenre almost feel_like a secret precedent\n",
      "\n",
      "listen to any of the canonical bristol scene album of the mid late_’90s when the genre be start to chafe_against its boundary and you’d_think the claustrophobic anxious 21st_century start a few_year ahead of schedule\n",
      "\n",
      "look_at from the right_angle trip_hop be part of an_unbroken chain that run from the abrasion of ’80s post_punk to the ruminative pop r&b dance fusion of the moment\n",
      "\n",
      "the best of it have age far_more gracefully and forcefully than_anything record in the waning_day of the record_industry ’s pre filesharing monomania have any right to\n",
      "\n",
      "tricky rebel_against be attach at the hip to a scene he be already look to shed and decamp for jamaica to record a more_aggressive bristle energy mutation of his style in ’96 the name pre_millennium tension be the only obvious thing that tell_you it ’s two_decade old rather_than two_week\n",
      "\n",
      "and portishead ’s ’97 self_title saw the stress fractured voice of beth_gibbons envision romance as codependent mutually_assured_destruction while geoff_barrow sink_into his rza noir beat like the conversation ’s gene hackman ruminate over his surveillance tape\n",
      "\n",
      "this be raw_nerved music too single_minded and intense to carry an obvious timestamp\n",
      "\n",
      "but massive_attack be the origin point of the trip_hop movement they and their_peer be strive_to escape the orbit of and they nearly tear themselves to shred in the process instead— or maybe as a result—they lay_down their go nova genre definitive paranoia statement with mezzanine\n",
      "\n",
      "the band third_album not count the mad_professor remixed no protection complete the last in a sort of de_facto bristol trilogy where tricky ’s youthful iconoclasm and portishead ’s deep focus emotional_intensity set the scene for massive_attack ’s sense of near suffocate dread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 0, 10):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join('trigram_transformed_reviews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp.vocab[\"'s\"].is_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda2\\envs\\py3DataScience\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "    for parsed_review in nlp.pipe(line_review(review_text_path), batch_size=10000, n_threads=3):\n",
    "        unigram_review = [token.lemma_ for token in parsed_review if not punct_space(token)]\n",
    "        \n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "        \n",
    "        trigram_review = [term for term in trigram_review if term not in spacy.en.STOPWORDS and len(term) > 2]\n",
    "        \n",
    "        trigram_review = u' '.join(trigram_review)\n",
    "        f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "On 2006’s “That’s Life,” Killer Mike boasted “You’d be hard-pressed to find another rapper smart as me,” opening up about Cornel West and Michael Eric Dyson, poverty, respectability politics, and civil rights, before taking on both Bush Administrations (“George Bush don’t like blacks … and his daddy CIA had flooded the hood with rock”). A few months later, El-P was waging war with the same enemy in the 9/11 conspiracy theory thriller “Run the Numbers,” concluding that “it always comes back to a Bush.” The two songs sounded very little alike, but the music (and the rappers) shared a similar fire and presence: confident, conspiratorial, no-holds-barred, and razor-sharp. Neither were likely to be deemed “political” rappers then, but both were already dissenters and nonconformists; independent artists signed to themselves, free thinkers shooting off at the mouth. Nearly a decade after airing out the Bush family, the duo, as Run the Jewels, have found a creative renaissance. The group’s latest self-titled album, Run the Jewels 3, is a well timed, finely tuned rap epic that confronts the ruling class (here addressed as “the masters”) with deadly precision; it’s rap as resistance.With a demagogue waiting in the wings to assume the presidency, their particular Molotov mix of explosive shit-talking and unfiltered insubordination feels vital.Their interplay is instinctual this time around; the songs move and shuffle with its MCs intuitively trading bars, filling the gaps in each others’ phrases, and feeding off each others’ energies, using their booming voices to cut through the startling noises of a future dystopia. “Poor folk love us the rich hate our faces/We talk too loud, won't remain in our places,” El-P raps on “Everybody Stay Calm.” They’re both observers who refuse to sugarcoat. “I just try my best, man, to say something about the shit I see,” Killer Mike told The New Republic in 2015. “Because I don’t want to go crazy. I don’t want to be walking around angry and feeling rage.” To that end, RTJ3 isn’t a response or reaction, it’s a preemptive strike, laying the groundwork for the battleground ahead. Their methods remain consistent, but the stakes have been raised over the years. RTJ1 was a fun experiment; RTJ2 was a classicist statement, and now RTJ3 is a reckoning. Many of these songs have more urgency than before; If RTJ2 was the music of protest, then this is the music of revolt. In that way, RTJ3 is essentially the Run the Jewels manifesto, an outpouring of rage and defiance that is never overcome by the moment and never loses sight of the objectives: rallying the troops, holding everyone accountable (from lawmakers, to other rappers, to Don Lemon and themselves), and toppling oppression wherever it may reign (on “Thieves! (Screamed the Ghost),” El-P raps, “Fear’s been law for so long rage feels like therapy”). “Thursday in the Danger Room” peers into the duo's personal turmoil and their shared history, and on “2100” Killer Mike lays out their President-Trump survival strategy: “You defeat the devil when you hold onto hope.” The key to RTJ3 is closer “A Report to the Shareholders,” which is plainspoken about the duo’s message and intent: “Maybe that’s why me and Mike get along / Not from the same part of town, but we both hear the same sound coming / And it sounds like war.” Seconds later, Killer Mike goes full Malcolm X: “Choose the lesser of the evil people, and the devil still gon’ win / It could all be over tomorrow, kill our masters and start again.” This is the ire of a group that’s tired of saying I told you so.This is by far the best produced record of their trilogy, with beats that find new and interesting ways to wreak havoc. “Call Ticketron” turns automated ticketing technology into a beacon for alien transmissions. On “Hey Kids (Bumaye)” crackling static and thumping bass crater open to reveal whirring, wobbling tones and ghostly whispers, and Danny Brown slots in an exceptional guest verse. On “Panther Like a Panther (Miracle Mix),” furnished by the shouts of Miami rap goddess Trina, rounded blips mimic the patter of hand drums before bursting into a wave of buzzing, distorted noise that slowly dissipates back into nothing. They’re still clearly having fun doing this and it’s still fun to listen to them work.It isn’t quite as punchy as RTJ2, which was brutish in its tactics, with nonstop bangs and thrills, but RTJ3 is a triumph in its own right that somehow celebrates the success of a seemingly unlikely friendship and mourns the collapse of a nation all at once. “Thieves! (Screamed the Ghost),” a song about riots as a response to violence as opposed to a means to create it, samples an iconic Martin Luther King, Jr. quote from the 1967 speech “The Other America”: “A riot is the language of the unheard.” In keeping with that idea, RTJ3 is a soundtrack for the riots to come.\n",
      "\n",
      "-------\n",
      "\n",
      "Transformed:\n",
      "\n",
      "2006 life killer_mike boast you’d hard_press_to rapper smart open_up cornel west michael eric dyson poverty respectability politics civil_right bush administrations george_bush don’t like black daddy cia flood hood rock few_month_later el_p wag_war enemy 9/11 conspiracy_theory thriller run numbers conclude come bush song sound very_little alike music rapper share similar presence confident conspiratorial hold bar razor_sharp likely deem political rapper dissenter nonconformist independent_artist sign free thinker shoot mouth nearly decade air bush family duo run jewels creative_renaissance group late self_title album run jewels time finely_tune rap epic confront ruling_class address master deadly precision rap resistance demagogue wait wing assume presidency particular molotov mix explosive shit_talk unfiltered insubordination feel vital interplay instinctual time song shuffle intuitively trade bar gap phrase feed_off energy use booming_voice cut_through startling noise future dystopia poor folk love rich hate faces/we talk too_loud wo_not remain place el_p rap everybody stay_calm they’re observer refuse_to sugarcoat try best man shit killer_mike tell new republic 2015 i_don’t_want go_crazy i_don’t_want walk_around angry feel rage end rtj3 isn’t response reaction it_’ preemptive_strike lay groundwork_for battleground ahead method remain consistent stake raise year rtj1 fun experiment rtj2 classicist statement rtj3 reckoning these_song urgency rtj2 music protest music revolt way rtj3 essentially run jewels manifesto outpouring rage defiance overcome moment never_lose_sight objective rally troop hold accountable lawmaker rapper lemon topple oppression reign thief scream ghost el_p rap fear law long rage feel_like therapy thursday danger room peer duo personal turmoil share history 2100 killer_mike lay_out president trump survival strategy defeat devil hold_onto hope key rtj3 close report shareholder plainspoken duo message intent maybe mike town hear sound come sound_like war second later killer_mike malcolm_x choose evil people devil gon win tomorrow kill master start ire group tired tell_you far best produce record trilogy beat new interesting way wreak_havoc ticketron turn automated ticketing technology beacon alien transmission hey kid bumaye crackle_static thumping_bass crater open reveal whirring wobble tone ghostly whisper danny_brown slot an_exceptional guest_verse panther like panther miracle mix furnish shout miami rap goddess trina round blip mimic patter hand_drum burst_into wave buzz distort noise slowly dissipate they’re clearly fun it_’ fun listen work isn’t_quite punchy rtj2 brutish tactic nonstop bang thrill rtj3 triumph its_own_right celebrate success seemingly unlikely friendship mourn collapse nation all_at_once thief scream ghost song riot response violence as_oppose_to mean create sample iconic martin_luther_king_jr. quote 1967 speech america riot language unheard idea rtj3 soundtrack riot come\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u'Original:' +u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_text_path), 11, 12):\n",
    "    print(review)\n",
    "    \n",
    "print(u'-------' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf-8') as f:\n",
    "    for review in it.islice(f, 11, 12):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "    from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "trigram_dictionary_filepath = os.path.join('trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# remove stop_words\n",
    "\n",
    "stop = ['.',',','(',')',\"'\",'\"',':','','...','-','``',';',\";'\",'&']\n",
    "\n",
    "stop += [\"'s\", \"’s\",\n",
    "         \"n't\", \"n’t\",\n",
    "         \"there_'\", \"there_’\",\n",
    "         \"they_'re\", \"they_’re\",\n",
    "         \"he_'\", \"he_’\", \n",
    "         \"it_’\", \"it_'\"]\n",
    "\n",
    "\n",
    "stop = set(stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "\n",
    "trigram_dictionary.filter_tokens(stop)\n",
    "\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join('trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_reviews_filepath))\n",
    "\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join('lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    lda = LdaMulticore(trigram_bow_corpus,\n",
    "                      num_topics=50,\n",
    "                      id2word=trigram_dictionary,\n",
    "                      workers=3)\n",
    "lda.save(lda_model_filepath)\n",
    "    \n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "       \n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "love                 0.004\n",
      "pop                  0.004\n",
      "he_'                 0.003\n",
      "style                0.003\n",
      "there_'              0.002\n",
      "rock                 0.002\n",
      "group                0.002\n",
      "single               0.002\n",
      "little               0.002\n",
      "sing                 0.002\n",
      "sound_like           0.002\n",
      "lyric                0.002\n",
      "use                  0.002\n",
      "great                0.002\n",
      "beat                 0.002\n",
      "hear                 0.002\n",
      "moment               0.002\n",
      "know                 0.002\n",
      "mix                  0.002\n",
      "place                0.002\n",
      "year                 0.002\n",
      "it_’                 0.002\n",
      "write                0.002\n",
      "voice                0.002\n",
      "mean                 0.001\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trigram_dict_all.dict'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_dictionary_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3DataScience]",
   "language": "python",
   "name": "conda-env-py3DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
