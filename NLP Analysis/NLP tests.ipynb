{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate spacy parser\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pull out sample text from review database\n",
    "df = pd.read_csv('content.csv')\n",
    "sample = df.content\n",
    "sample = sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse sample text with spacy object\n",
    "parsed_review = nlp(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "“Trip-hop” eventually became a ’90s punchline, a music-press shorthand for “overhyped hotel lounge music.\n",
      "\n",
      "Sentence 2:\n",
      "” But today, the much-maligned subgenre almost feels like a secret precedent.\n",
      "\n",
      "Sentence 3:\n",
      "Listen to any of the canonical Bristol-scene albums of the mid-late ’90s, when the genre was starting to chafe against its boundaries, and you’d think the claustrophobic, anxious 21st century started a few years ahead of schedule.\n",
      "\n",
      "Sentence 4:\n",
      "Looked at from the right angle, trip-hop is part of an unbroken chain that runs from the abrasion of ’80s post-punk to the ruminative pop-R&B-dance fusion of the moment. \n",
      "\n",
      "Sentence 5:\n",
      "The best of it has aged far more gracefully (and forcefully) than anything recorded in the waning days of the record industry’s pre-filesharing monomania has any right to.\n",
      "\n",
      "Sentence 6:\n",
      "Tricky rebelled against being attached at the hip to a scene he was already looking to shed and decamped for Jamaica to record a more aggressive, bristling-energy mutation of his style in ’96; the name Pre-Millennium Tension is the only obvious thing that tells you it’s two decades old rather than two weeks.\n",
      "\n",
      "Sentence 7:\n",
      "And Portishead’s ’97 self-titled saw the stress-fractured voice of Beth Gibbons envisioning romance as codependent, mutually assured destruction while Geoff Barrow sunk into his RZA-noir beats like The Conversation’s Gene Hackman ruminating over his surveillance tapes.\n",
      "\n",
      "Sentence 8:\n",
      "This was raw-nerved music, too single-minded and intense to carry an obvious timestamp. \n",
      "\n",
      "Sentence 9:\n",
      "But Massive Attack were the origin point of the trip-hop movement they and their peers were striving to escape the orbit of, and they nearly tore themselves to shreds in the process. Instead— or maybe as a result—they laid down their going-nova genre's definitive paranoia statement with Mezzanine.\n",
      "\n",
      "Sentence 10:\n",
      "The band's third album (not counting the Mad Professor-remixed No Protection) completes the last in a sort of de facto Bristol trilogy, where Tricky’s youthful iconoclasm and Portishead’s deep-focus emotional intensity set the scene for Massive Attack’s sense of near-suffocating dread.\n",
      "\n",
      "Sentence 11:\n",
      "The album corroded their tendencies to make big-wheel hymnals of interconnected lives where hope and despair trade precedent—on Mezzanine, it’s alienation all the way down.\n",
      "\n",
      "Sentence 12:\n",
      "There’s no safety from harm here, nothing you’ve got to be thankful for, nobody to take the force of the blow: what Mezzanine provides instead is a succession of parties and relationships and panopticons where the walls won’t stop closing in.\n",
      "\n",
      "Sentence 13:\n",
      "The lyrics establish this atmosphere all on their own.\n",
      "\n",
      "Sentence 14:\n",
      "Sex, in “Inertia Creeps,” is reduced to a meeting of “two undernourished egos, four rotating hips,” the focus of a failing relationship that's left its participants too numbed with their own routine dishonesty to break it off.\n",
      "\n",
      "Sentence 15:\n",
      "The voice singing it—Massive Attack's cornerstone co-writer/producer Robert “3D” Del Naja—is raspy from exhaustion.\n",
      "\n",
      "Sentence 16:\n",
      "“\n",
      "\n",
      "Sentence 17:\n",
      "Dissolved Girl” reiterates this theme from the perspective of guest vocalist Sarah Jay Hawley (“Passion’s overrated anyway”).\n",
      "\n",
      "Sentence 18:\n",
      "On “\n",
      "\n",
      "Sentence 19:\n",
      "Risingson,” Grant “Daddy G”\n",
      "\n",
      "Sentence 20:\n",
      "Marshall nails the boredom and anxiety of being stuck somewhere you can’t stand with someone you’re starting to feel the same way about (“Why you want to take me to this party and breathe/I’m dying to leave/Every time we grind you know we severed lines”).\n",
      "\n",
      "Sentence 21:\n",
      "But Mezzanine’s defining moments come from guest vocalists who were famous long before Massive Attack even released their first album.\n",
      "\n",
      "Sentence 22:\n",
      "Horace Andy was already a legend in reggae circles, but his collaborations with Massive Attack gave him a wider crossover exposure, and all three of his appearances on Mezzanine are homages or nods to songs he'd charted with in his early-’70s come-up.\n",
      "\n",
      "Sentence 23:\n",
      "“\n",
      "\n",
      "Sentence 24:\n",
      "Angel” is a loose rewrite of his 1973 single “\n",
      "\n",
      "Sentence 25:\n",
      "You Are My Angel,” but it’s a fakeout after the first verse—originally a vision of beauty (“Come from way above/To bring me love”), transformed into an Old Testament avenger: “\n",
      "\n",
      "Sentence 26:\n",
      "On the dark side/Neutralize every man in sight.\n",
      "\n",
      "Sentence 27:\n",
      "”\n",
      "\n",
      "Sentence 28:\n",
      "The parenthetically titled, album-closing reprise of “(Exchange)” is a ghostly invocation of Andy’s “See\n",
      "\n",
      "Sentence 29:\n",
      "A Man's Face” cleverly disguised as a comedown track.\n",
      "\n",
      "Sentence 30:\n",
      "And then there’s “\n",
      "\n",
      "Sentence 31:\n",
      "Man Next Door,” the John Holt standard that Andy had previously recorded as “\n",
      "\n",
      "Sentence 32:\n",
      "Quiet Place”—on Mezzanine, it sounds less like an overheard argument from the next apartment over and more like a close-quarters reckoning with violence heard through thin walls ready to break.\n",
      "\n",
      "Sentence 33:\n",
      "It’s Andy at his emotionally nuanced and evocative best.\n",
      "\n",
      "Sentence 34:\n",
      "The other outside vocalist was even more of a coup: Liz Fraser, the singer and songwriter of Cocteau Twins, lends her virtuoso soprano to three songs that feel like exorcisms of the personal strife accompanying her band\n",
      "\n",
      "Sentence 35:\n",
      "’s breakup.\n",
      "\n",
      "Sentence 36:\n",
      "Her voice serves as an ethereal counterpoint to speaker-rattling production around it.\n",
      "\n",
      "Sentence 37:\n",
      "“\n",
      "\n",
      "Sentence 38:\n",
      "Black Milk” contains the album’s most spiritually unnerving words (“Eat me/In the space/Within my heart/Love you for God/Love you for the Mother”), even as her lead and the elegiac beat make for some of its most beautiful sounds.\n",
      "\n",
      "Sentence 39:\n",
      "She provides the wistful counterpoint to the night-shift alienation of “Group Four.”\n",
      "\n",
      "Sentence 40:\n",
      "And then there's “Teardrop,”\n",
      "\n",
      "Sentence 41:\n",
      "her finest moment on the album.\n",
      "\n",
      "Sentence 42:\n",
      "Legend has it the song was briefly considered for Madonna; Andrew “Mushroom” Vowles sent the demo to her, but was overruled by Daddy G and 3D, who both wanted Fraser.\n",
      "\n",
      "Sentence 43:\n",
      "Democracy thankfully worked this time around, as Fraser’s performance—recorded in part on the day she discovered that Jeff Buckley, who she’d had an estranged working relationship and friendship with, had drowned in Memphis’\n",
      "\n",
      "Sentence 44:\n",
      "Wolf River—was a heart-rending performance that gave Massive Attack their first (and so far only)\n",
      "\n",
      "Sentence 45:\n",
      "UK Top 10 hit.\n",
      "\n",
      "Sentence 46:\n",
      "Originally set for a late ’97 release, Mezzanine got pushed back four months because Del Naja refused to stop reworking the tracks, tearing them apart and rebuilding them until they’re so polished they gleam.\n",
      "\n",
      "Sentence 47:\n",
      "It sure sounds like the product of bloody-knuckled labor, all that empty-space reverb and melted-together multitrack vocals and oppressive low-end.\n",
      "\n",
      "Sentence 48:\n",
      "(The first sound you hear on the album, that lead-jointed bassline on “\n",
      "\n",
      "Sentence 49:\n",
      "Angel,” is to subwoofers what “\n",
      "\n",
      "Sentence 50:\n",
      "Planet Earth” \n",
      "\n",
      "Sentence 51:\n",
      "is to high-def television.)\n",
      "\n",
      "Sentence 52:\n",
      "But it also groans with the burden of creative conflict, a working process that created rifts between Del Naja and Vowles, who left shortly after Mezzanine dropped following nearly 15 years of collaboration.\n",
      "\n",
      "Sentence 53:\n",
      "Mezzanine began the band’s relationship with producer Neil Davidge, who’d known Vowles dating back to the early ’90s and met the rest of the band after the completion of Protection.\n",
      "\n",
      "Sentence 54:\n",
      "He picked a chaotic time to jump in, but Davidge and 3D forged a creative bond working through that pressure. \n",
      "\n",
      "Sentence 55:\n",
      "Mezzanine was a document of unity, not fragmentation.\n",
      "\n",
      "Sentence 56:\n",
      "Despite their rifts, they were a post-genre outfit, one that couldn’t separate dub from punk from hip-hop from R&B because the basslines all worked together and because classifications are for toe tags.\n",
      "\n",
      "Sentence 57:\n",
      "All their acknowledged samples—including the joy-buzzer synths from Ultravox’s “Rockwrok” (“Inertia Creeps”), the opulent ache of Isaac Hayes’ celestial-soul take on “\n",
      "\n",
      "Sentence 58:\n",
      "Our Day Will Come” (“Exchange”), Robert Smith’s nervous “tick tick tick” from the Cure’s “10:15 Saturday Night,” and the most concrete-crumbling throwdown of the Led Zep “Levee” break ever deployed (the latter two on “\n",
      "\n",
      "Sentence 59:\n",
      "Man\n",
      "\n",
      "Sentence 60:\n",
      "Next Door”)—were sourced from  1968 and 1978, well-traveled crate-digging territory.\n",
      "\n",
      "Sentence 61:\n",
      "But what they build from that is its own beast.  \n",
      "\n",
      "Sentence 62:\n",
      "Their working method never got any faster.\n",
      "\n",
      "Sentence 63:\n",
      "The four-year gap between Protection and Mezzanine became a five-year gap until 2003’s 100th Window, then another seven years between that record and 2010’s Heligoland, plus another seven years and counting with no full-lengths to show for it.\n",
      "\n",
      "Sentence 64:\n",
      "Not that they've been slacking: we've gotten a multimedia film/music collaboration with Adam Curtis, the respectable but underrated Ritual Spirit EP, and Del Naja’s notoriously rumored side gig as Banksy.\n",
      "\n",
      "Sentence 65:\n",
      "(Hey, 3D does have a background in graffiti art.)\n",
      "\n",
      "Sentence 66:\n",
      "But the ordeal of both recording and touring Mezzanine took its own toll.\n",
      "\n",
      "Sentence 67:\n",
      "A late ’98 interview with Del Naja saw him optimistic about its reputation-shedding style: “\n",
      "\n",
      "Sentence 68:\n",
      "I always said it was for the greater good of the fucking project because if this album was a bit different from the last two, the next one would be even freer to be whatever it wants to be.”\n",
      "\n",
      "Sentence 69:\n",
      "But fatigue and restlessness rarely make for a productive mixture, and that same spark of tension which carried Mezzanine over the threshold proved unsustainable, not just for Massive Attack’s creativity but their continued existence.\n",
      "\n",
      "Sentence 70:\n",
      "Still, it’s hard not to feel the album’s legacy resonating elsewhere—and not just in “Teardrop” becoming the cue for millions of TV viewers to brace themselves for Hugh Laurie’s cranky-genius-doctor schtick. \n",
      "\n",
      "Sentence 71:\n",
      "Graft its tense feelings of nervy isolation and late-night melancholy onto two-step, and you’re partway to the blueprint for Plastician and Burial.\n",
      "\n",
      "Sentence 72:\n",
      "You can hear flashes of that mournful romantic alienation in James Blake, the graceful, bass-riddled emotional abrasion in FKA twigs, the all-absorbing post-genre rock/soul ambitions in Young Fathers or Algiers.\n",
      "\n",
      "Sentence 73:\n",
      "Mezzanine stands as an album built around echoes of the ’70s, wrestled through the immediacy of its creators' tumultuous late ’90s, and fearless enough that it still sounds like it belongs in whatever timeframe you're playing it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spacy can separate out sentences as below\n",
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print('Sentence {}:'.format(num+1))\n",
    "    print(sentence)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: today - DATE\n",
      "\n",
      "Entity 2: Bristol - GPE\n",
      "\n",
      "Entity 3: 21st century - DATE\n",
      "\n",
      "Entity 4: Jamaica - GPE\n",
      "\n",
      "Entity 5: ’96 - GPE\n",
      "\n",
      "Entity 6: two decades - DATE\n",
      "\n",
      "Entity 7: two weeks - DATE\n",
      "\n",
      "Entity 8: Portishead’s - ORG\n",
      "\n",
      "Entity 9: Beth Gibbons - PERSON\n",
      "\n",
      "Entity 10: Geoff Barrow - PERSON\n",
      "\n",
      "Entity 11: The Conversation’s Gene Hackman - WORK_OF_ART\n",
      "\n",
      "Entity 12: Mezzanine - PERSON\n",
      "\n",
      "Entity 13: third - ORDINAL\n",
      "\n",
      "Entity 14: Bristol - GPE\n",
      "\n",
      "Entity 15: Tricky’s - PERSON\n",
      "\n",
      "Entity 16: Portishead’s - PERSON\n",
      "\n",
      "Entity 17: Mezzanine - PERSON\n",
      "\n",
      "Entity 18: “Inertia Creeps - ORG\n",
      "\n",
      "Entity 19: two - CARDINAL\n",
      "\n",
      "Entity 20: four - CARDINAL\n",
      "\n",
      "Entity 21: Robert “3D - PERSON\n",
      "\n",
      "Entity 22: Del Naja—is - PERSON\n",
      "\n",
      "Entity 23: Sarah Jay - PERSON\n",
      "\n",
      "Entity 24: Risingson - PERSON\n",
      "\n",
      "Entity 25: Grant “ - PERSON\n",
      "\n",
      "Entity 26: Marshall - PERSON\n",
      "\n",
      "Entity 27: Mezzanine - NORP\n",
      "\n",
      "Entity 28: first - ORDINAL\n",
      "\n",
      "Entity 29: Horace Andy  - PERSON\n",
      "\n",
      "Entity 30: three - CARDINAL\n",
      "\n",
      "Entity 31: early-’70s - CARDINAL\n",
      "\n",
      "Entity 32: Angel” - PERSON\n",
      "\n",
      "Entity 33: 1973 - DATE\n",
      "\n",
      "Entity 34: Angel - PERSON\n",
      "\n",
      "Entity 35: first - ORDINAL\n",
      "\n",
      "Entity 36: “(Exchange - ORG\n",
      "\n",
      "Entity 37: Andy’s - PERSON\n",
      "\n",
      "Entity 38: ’s “ - ORG\n",
      "\n",
      "Entity 39: John Holt - PERSON\n",
      "\n",
      "Entity 40: Andy - PERSON\n",
      "\n",
      "Entity 41: Andy - PERSON\n",
      "\n",
      "Entity 42: Liz Fraser - PERSON\n",
      "\n",
      "Entity 43: Cocteau Twins - ORG\n",
      "\n",
      "Entity 44: three - CARDINAL\n",
      "\n",
      "Entity 45: Teardrop - ORG\n",
      "\n",
      "Entity 46: Madonna - PERSON\n",
      "\n",
      "Entity 47: Andrew “Mushroom - PERSON\n",
      "\n",
      "Entity 48: Daddy G - ORG\n",
      "\n",
      "Entity 49: Fraser - PERSON\n",
      "\n",
      "Entity 50: Fraser’s - PERSON\n",
      "\n",
      "Entity 51: Jeff Buckley - PERSON\n",
      "\n",
      "Entity 52: Memphis - GPE\n",
      "\n",
      "Entity 53: Wolf River—was - PERSON\n",
      "\n",
      "Entity 54: first - ORDINAL\n",
      "\n",
      "Entity 55: Mezzanine - ORG\n",
      "\n",
      "Entity 56: four months - DATE\n",
      "\n",
      "Entity 57: Del Naja - PERSON\n",
      "\n",
      "Entity 58: first - ORDINAL\n",
      "\n",
      "Entity 59: Angel - PERSON\n",
      "\n",
      "Entity 60: Del Naja - PERSON\n",
      "\n",
      "Entity 61: Vowles - PERSON\n",
      "\n",
      "Entity 62: Mezzanine - PERSON\n",
      "\n",
      "Entity 63: nearly 15 years - DATE\n",
      "\n",
      "Entity 64: Neil Davidge - PERSON\n",
      "\n",
      "Entity 65: Vowles - PERSON\n",
      "\n",
      "Entity 66: Davidge - PERSON\n",
      "\n",
      "Entity 67: Ultravox - GPE\n",
      "\n",
      "Entity 68: Isaac Hayes - PERSON\n",
      "\n",
      "Entity 69: Robert Smith - PERSON\n",
      "\n",
      "Entity 70: Saturday - DATE\n",
      "\n",
      "Entity 71: the latter - DATE\n",
      "\n",
      "Entity 72: 1968 - DATE\n",
      "\n",
      "Entity 73: 1978 - DATE\n",
      "\n",
      "Entity 74: four-year - DATE\n",
      "\n",
      "Entity 75: five-year - DATE\n",
      "\n",
      "Entity 76: 2003 - DATE\n",
      "\n",
      "Entity 77: 2010 - DATE\n",
      "\n",
      "Entity 78: Adam Curtis - PERSON\n",
      "\n",
      "Entity 79: Del Naja - PERSON\n",
      "\n",
      "Entity 80: Banksy - PERSON\n",
      "\n",
      "Entity 81: Mezzanine - ORG\n",
      "\n",
      "Entity 82: Del Naja - PERSON\n",
      "\n",
      "Entity 83: the last two - DATE\n",
      "\n",
      "Entity 84: Mezzanine - PERSON\n",
      "\n",
      "Entity 85: millions - CARDINAL\n",
      "\n",
      "Entity 86: Hugh Laurie’s - PERSON\n",
      "\n",
      "Entity 87: late-night - TIME\n",
      "\n",
      "Entity 88: two - CARDINAL\n",
      "\n",
      "Entity 89: Plastician - NORP\n",
      "\n",
      "Entity 90: Burial - GPE\n",
      "\n",
      "Entity 91: James Blake - PERSON\n",
      "\n",
      "Entity 92: Algiers - GPE\n",
      "\n",
      "Entity 93: late ’90s - TIME\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# named entity detection\n",
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print('Entity {}:'.format(num+1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trip</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hop</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token_text part_of_speech\n",
       "0          “          PUNCT\n",
       "1       Trip           NOUN\n",
       "2          -          PUNCT\n",
       "3        hop           NOUN\n",
       "4          ”          PUNCT"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token identification\n",
    "\n",
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "token_df = pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "            columns=['token_text', 'part_of_speech'])\n",
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“</td>\n",
       "      <td>\"</td>\n",
       "      <td>“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trip</td>\n",
       "      <td>trip</td>\n",
       "      <td>Xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hop</td>\n",
       "      <td>hop</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "      <td>\"</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eventually</td>\n",
       "      <td>eventually</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>became</td>\n",
       "      <td>become</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>’90s</td>\n",
       "      <td>’90s</td>\n",
       "      <td>’ddx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>punchline</td>\n",
       "      <td>punchline</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>press</td>\n",
       "      <td>press</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shorthand</td>\n",
       "      <td>shorthand</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>“</td>\n",
       "      <td>\"</td>\n",
       "      <td>“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>overhyped</td>\n",
       "      <td>overhyped</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hotel</td>\n",
       "      <td>hotel</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text token_lemma token_shape\n",
       "0            “           \"           “\n",
       "1         Trip        trip        Xxxx\n",
       "2            -           -           -\n",
       "3          hop         hop         xxx\n",
       "4            ”           \"           ”\n",
       "5   eventually  eventually        xxxx\n",
       "6       became      become        xxxx\n",
       "7            a           a           x\n",
       "8         ’90s        ’90s        ’ddx\n",
       "9    punchline   punchline        xxxx\n",
       "10           ,           ,           ,\n",
       "11           a           a           x\n",
       "12       music       music        xxxx\n",
       "13           -           -           -\n",
       "14       press       press        xxxx\n",
       "15   shorthand   shorthand        xxxx\n",
       "16         for         for         xxx\n",
       "17           “           \"           “\n",
       "18   overhyped   overhyped        xxxx\n",
       "19       hotel       hotel        xxxx"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text normalization, stemming/lemmatization\n",
    "# shape analysis\n",
    "\n",
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "nor_df = pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "                     columns=['token_text', 'token_lemma', 'token_shape'])\n",
    "nor_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trip</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hop</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eventually</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>became</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>’90s</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>punchline</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>music</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>press</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shorthand</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>“</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>overhyped</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hotel</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text entity_type inside_outside_begin\n",
       "0            “                                O\n",
       "1         Trip                                O\n",
       "2            -                                O\n",
       "3          hop                                O\n",
       "4            ”                                O\n",
       "5   eventually                                O\n",
       "6       became                                O\n",
       "7            a                                O\n",
       "8         ’90s                                O\n",
       "9    punchline                                O\n",
       "10           ,                                O\n",
       "11           a                                O\n",
       "12       music                                O\n",
       "13           -                                O\n",
       "14       press                                O\n",
       "15   shorthand                                O\n",
       "16         for                                O\n",
       "17           “                                O\n",
       "18   overhyped                                O\n",
       "19       hotel                                O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token-level entity analysis\n",
    "\n",
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "ent_df = pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "                     columns=['token_text', 'entity_type', 'inside_outside_begin'])\n",
    "ent_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_proba</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“</td>\n",
       "      <td>-9.795314</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trip</td>\n",
       "      <td>-13.672223</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.468655</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hop</td>\n",
       "      <td>-10.939725</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>”</td>\n",
       "      <td>-9.812149</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eventually</td>\n",
       "      <td>-9.494384</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>became</td>\n",
       "      <td>-9.810510</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.929788</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>’90s</td>\n",
       "      <td>-18.391684</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>punchline</td>\n",
       "      <td>-12.871519</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.454960</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.929788</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>music</td>\n",
       "      <td>-8.735067</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.468655</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>press</td>\n",
       "      <td>-10.130120</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shorthand</td>\n",
       "      <td>-13.740866</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>for</td>\n",
       "      <td>-4.880109</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>“</td>\n",
       "      <td>-9.795314</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>overhyped</td>\n",
       "      <td>-13.863889</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hotel</td>\n",
       "      <td>-11.047312</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lounge</td>\n",
       "      <td>-12.681898</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>music</td>\n",
       "      <td>-8.735067</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.067898</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>”</td>\n",
       "      <td>-9.812149</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>But</td>\n",
       "      <td>-7.014297</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>today</td>\n",
       "      <td>-8.664745</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.454960</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.528767</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>much</td>\n",
       "      <td>-6.584302</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.468655</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  log_proba stop? punctuation? whitespace? number?  \\\n",
       "0            “  -9.795314                Yes                       \n",
       "1         Trip -13.672223                                          \n",
       "2            -  -5.468655                Yes                       \n",
       "3          hop -10.939725                                          \n",
       "4            ”  -9.812149                Yes                       \n",
       "5   eventually  -9.494384                                          \n",
       "6       became  -9.810510   Yes                                    \n",
       "7            a  -3.929788   Yes                                    \n",
       "8         ’90s -18.391684                                          \n",
       "9    punchline -12.871519                                          \n",
       "10           ,  -3.454960                Yes                       \n",
       "11           a  -3.929788   Yes                                    \n",
       "12       music  -8.735067                                          \n",
       "13           -  -5.468655                Yes                       \n",
       "14       press -10.130120                                          \n",
       "15   shorthand -13.740866                                          \n",
       "16         for  -4.880109   Yes                                    \n",
       "17           “  -9.795314                Yes                       \n",
       "18   overhyped -13.863889                                          \n",
       "19       hotel -11.047312                                          \n",
       "20      lounge -12.681898                                          \n",
       "21       music  -8.735067                                          \n",
       "22           .  -3.067898                Yes                       \n",
       "23           ”  -9.812149                Yes                       \n",
       "24         But  -7.014297   Yes                                    \n",
       "25       today  -8.664745                                          \n",
       "26           ,  -3.454960                Yes                       \n",
       "27         the  -3.528767   Yes                                    \n",
       "28        much  -6.584302   Yes                                    \n",
       "29           -  -5.468655                Yes                       \n",
       "\n",
       "   out of vocab.?  \n",
       "0                  \n",
       "1                  \n",
       "2                  \n",
       "3                  \n",
       "4                  \n",
       "5                  \n",
       "6                  \n",
       "7                  \n",
       "8                  \n",
       "9                  \n",
       "10                 \n",
       "11                 \n",
       "12                 \n",
       "13                 \n",
       "14                 \n",
       "15                 \n",
       "16                 \n",
       "17                 \n",
       "18                 \n",
       "19                 \n",
       "20                 \n",
       "21                 \n",
       "22                 \n",
       "23                 \n",
       "24                 \n",
       "25                 \n",
       "26                 \n",
       "27                 \n",
       "28                 \n",
       "29                 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                    token.prob,\n",
    "                    token.is_stop,\n",
    "                    token.is_punct,\n",
    "                    token.is_space,\n",
    "                    token.like_num,\n",
    "                    token.is_oov)\n",
    "                   for token in parsed_review]\n",
    "\n",
    "df1 = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                          'log_proba',\n",
    "                          'stop?',\n",
    "                          'punctuation?',\n",
    "                          'whitespace?',\n",
    "                          'number?',\n",
    "                          'out of vocab.?'])\n",
    "\n",
    "def fill_in(x):\n",
    "    if x:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df1.loc[:, 'stop?':'out of vocab.?'] = (df1.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "df1.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda2\\envs\\py3DataScience\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "review_text_path = os.path.join('review_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = os.path.join('unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "if 0 == 1:\n",
    "    with codecs.open(unigram_sentences, 'w', encoding='utf-8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_text_path):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_parser = LineSentence(unigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip hop eventually become a ’90s punchline a music press shorthand for overhyped hotel lounge music\n",
      "\n",
      "but today the much malign subgenre almost feel like a secret precedent\n",
      "\n",
      "listen to any of the canonical bristol scene album of the mid late ’90s when the genre be start to chafe against its boundary and you’d think the claustrophobic anxious 21st century start a few year ahead of schedule\n",
      "\n",
      "look at from the right angle trip hop be part of an unbroken chain that run from the abrasion of ’80s post punk to the ruminative pop r&b dance fusion of the moment\n",
      "\n",
      "the best of it have age far more gracefully and forcefully than anything record in the waning day of the record industry ’s pre filesharing monomania have any right to\n",
      "\n",
      "tricky rebel against be attach at the hip to a scene he be already look to shed and decamp for jamaica to record a more aggressive bristle energy mutation of his style in ’96 the name pre millennium tension be the only obvious thing that tell you it ’s two decade old rather than two week\n",
      "\n",
      "and portishead ’s ’97 self title saw the stress fractured voice of beth gibbons envision romance as codependent mutually assured destruction while geoff barrow sink into his rza noir beat like the conversation ’s gene hackman ruminate over his surveillance tape\n",
      "\n",
      "this be raw nerved music too single minded and intense to carry an obvious timestamp\n",
      "\n",
      "but massive attack be the origin point of the trip hop movement they and their peer be strive to escape the orbit of and they nearly tear themselves to shred in the process instead— or maybe as a result—they lay down their go nova genre 's definitive paranoia statement with mezzanine\n",
      "\n",
      "the band 's third album not count the mad professor remixed no protection complete the last in a sort of de facto bristol trilogy where tricky ’s youthful iconoclasm and portishead ’s deep focus emotional intensity set the scene for massive attack ’s sense of near suffocate dread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "for unigram_sentence in it.islice(unigram_sentences_parser, 0, 10):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join('bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases(unigram_sentences_parser)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join('bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda2\\envs\\py3DataScience\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "if 1 == 1:\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf-8') as f:\n",
    "        for unigram_sentence in unigram_sentences_parser:\n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_hop eventually_become a ’90s punchline a music press shorthand_for overhyped hotel_lounge music\n",
      "\n",
      "but today the much_malign subgenre almost feel_like a secret precedent\n",
      "\n",
      "listen to any of the canonical bristol scene album of the mid late_’90s when the genre be start to chafe_against its boundary and you’d_think the claustrophobic anxious 21st_century start a few_year ahead of schedule\n",
      "\n",
      "look_at from the right_angle trip_hop be part of an unbroken chain that run from the abrasion of ’80s post_punk to the ruminative pop r&b dance fusion of the moment\n",
      "\n",
      "the best of it have age far more gracefully and forcefully than_anything record in the waning_day of the record industry ’s pre filesharing monomania have any right to\n",
      "\n",
      "tricky rebel_against be attach at the hip to a scene he be already look to shed and decamp for jamaica to record a more aggressive bristle energy mutation of his style in ’96 the name pre_millennium tension be the only obvious thing that tell you it ’s two_decade old rather_than two week\n",
      "\n",
      "and portishead ’s ’97 self_title saw the stress fractured voice of beth_gibbons envision romance as codependent mutually_assured destruction while geoff_barrow sink_into his rza noir beat like the conversation ’s gene hackman ruminate over his surveillance tape\n",
      "\n",
      "this be raw_nerved music too single_minded and intense to carry an obvious timestamp\n",
      "\n",
      "but massive_attack be the origin point of the trip_hop movement they and their peer be strive to escape the orbit of and they nearly tear themselves to shred in the process instead— or maybe as a result—they lay_down their go nova genre 's definitive paranoia statement with mezzanine\n",
      "\n",
      "the band 's third_album not count the mad_professor remixed no protection complete the last in a sort of de_facto bristol trilogy where tricky ’s youthful iconoclasm and portishead ’s deep focus emotional_intensity set the scene for massive_attack ’s sense of near suffocate dread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 0, 10):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join('trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "trigram_model.save(trigram_model_filepath)\n",
    "\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join('trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_hop eventually_become a ’90s punchline a music press shorthand_for overhyped hotel_lounge music\n",
      "\n",
      "but today the much_malign subgenre almost feel_like a secret precedent\n",
      "\n",
      "listen to any of the canonical bristol scene album of the mid late_’90s when the genre be start to chafe_against its boundary and you’d_think the claustrophobic anxious 21st_century start a few_year ahead of schedule\n",
      "\n",
      "look_at from the right_angle trip_hop be part of an_unbroken chain that run from the abrasion of ’80s post_punk to the ruminative pop r&b dance fusion of the moment\n",
      "\n",
      "the best of it have age far_more gracefully and forcefully than_anything record in the waning_day of the record_industry ’s pre filesharing monomania have any right to\n",
      "\n",
      "tricky rebel_against be attach at the hip to a scene he be already look to shed and decamp for jamaica to record a more_aggressive bristle energy mutation of his style in ’96 the name pre_millennium tension be the only obvious thing that tell_you it ’s two_decade old rather_than two_week\n",
      "\n",
      "and portishead ’s ’97 self_title saw the stress fractured voice of beth_gibbons envision romance as codependent mutually_assured_destruction while geoff_barrow sink_into his rza noir beat like the conversation ’s gene hackman ruminate over his surveillance tape\n",
      "\n",
      "this be raw_nerved music too single_minded and intense to carry an obvious timestamp\n",
      "\n",
      "but massive_attack be the origin point of the trip_hop movement they and their_peer be strive_to escape the orbit of and they nearly tear themselves to shred in the process instead— or maybe as a result—they lay_down their go nova genre 's definitive paranoia statement with mezzanine\n",
      "\n",
      "the band 's third_album not count the mad_professor remixed no protection complete the last in a sort of de_facto bristol trilogy where tricky ’s youthful iconoclasm and portishead ’s deep focus emotional_intensity set the scene for massive_attack ’s sense of near suffocate dread\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 0, 10):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join('trigram_transformed_reviews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.vocab[\"'s\"].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda2\\envs\\py3DataScience\\lib\\site-packages\\gensim\\models\\phrases.py:316: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(trigram_reviews_filepath, 'w', encoding='utf-8') as f:\n",
    "    for parsed_review in nlp.pipe(line_review(review_text_path), batch_size=10000, n_threads=4):\n",
    "        unigram_review = [token.lemma_ for token in parsed_review if not punct_space(token)]\n",
    "        \n",
    "        bigram_review = bigram_model[unigram_review]\n",
    "        trigram_review = trigram_model[bigram_review]\n",
    "        \n",
    "        trigram_review = [term for term in trigram_review if term not in spacy.en.STOPWORDS]\n",
    "        \n",
    "        trigram_review = u' '.join(trigram_review)\n",
    "        f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(u'Original:' +u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_text_path), 11, 12):\n",
    "    print(review)\n",
    "    \n",
    "print(u'-------' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf-8') as f:\n",
    "    for review in it.islice(f, 11, 12):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "    from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "trigram_dictionary_filepath = os.path.join('trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "\n",
    "trigram_dictionary.filter_tokens(bad_ids=(u\"'\", u\"'s\", u\"it_'\"))\n",
    "\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join('trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_reviews_filepath))\n",
    "\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join('lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    lda = LdaMulticore(trigram_bow_corpus,\n",
    "                      num_topics=50,\n",
    "                      id2word=trigram_dictionary,\n",
    "                      workers=3)\n",
    "lda.save(lda_model_filepath)\n",
    "    \n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "       \n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "he_'                 0.003\n",
      "’s                   0.003\n",
      "love                 0.003\n",
      "they_'re             0.002\n",
      "little               0.002\n",
      "there_'              0.002\n",
      "try                  0.002\n",
      "voice                0.002\n",
      "pop                  0.002\n",
      "set                  0.002\n",
      "hear                 0.002\n",
      "mix                  0.002\n",
      "know                 0.002\n",
      "lyric                0.002\n",
      "kind                 0.002\n",
      "sing                 0.002\n",
      "style                0.002\n",
      "hip_hop              0.002\n",
      "rock                 0.002\n",
      "beat                 0.002\n",
      "sound_like           0.002\n",
      "moment               0.002\n",
      "place                0.001\n",
      "year                 0.001\n",
      "turn                 0.001\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trigram_dict_all.dict'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_dictionary_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3DataScience]",
   "language": "python",
   "name": "conda-env-py3DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
